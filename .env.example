# =============================================================================
# Go AI Agents Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your actual values

# =============================================================================
# LLM Provider API Keys
# =============================================================================

# OpenAI API Key (required for OpenAI models)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic API Key (required for Claude models)  
# Get your key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# =============================================================================
# Server Configuration
# =============================================================================

# HTTP server port
HTTP_PORT=8080

# Enable CORS for web applications
ENABLE_CORS=true

# Enable debug mode
DEBUG=false

# Log level (debug, info, warn, error)
LOG_LEVEL=info

# =============================================================================
# LLM Configuration
# =============================================================================

# Default model to use (can be overridden per request)
# OpenAI options: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
# Anthropic options: claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022, claude-3-opus-20240229
DEFAULT_MODEL=gpt-4o-mini

# Default temperature (0.0 to 2.0 for OpenAI, 0.0 to 1.0 for Anthropic)
DEFAULT_TEMPERATURE=0.7

# Default max tokens per response
DEFAULT_MAX_TOKENS=1000

# LLM request timeout (e.g., 30s, 1m, 2m30s)
LLM_TIMEOUT=30s

# =============================================================================
# Retry Configuration
# =============================================================================

# Maximum number of retries for failed requests
MAX_RETRIES=3

# Initial delay before first retry
INITIAL_DELAY=1s

# Maximum delay between retries  
MAX_DELAY=60s

# Backoff multiplier (exponential backoff)
BACKOFF_FACTOR=2.0

# =============================================================================
# Memory Configuration
# =============================================================================

# Memory store type (inmemory, redis)
MEMORY_STORE_TYPE=inmemory

# Redis connection string (if using Redis memory store)
# REDIS_URL=redis://localhost:6379

# Redis database number
# REDIS_DB=0

# Redis password (if required)
# REDIS_PASSWORD=your_redis_password

# =============================================================================
# Agent Configuration
# =============================================================================

# Default system prompt for agents
DEFAULT_SYSTEM_PROMPT="You are a helpful AI assistant. Be concise and accurate in your responses."

# Maximum number of reasoning iterations per agent run
MAX_AGENT_ITERATIONS=10

# Agent request timeout
AGENT_TIMEOUT=5m

# =============================================================================
# Observability Configuration
# =============================================================================

# Enable metrics collection
ENABLE_METRICS=true

# Metrics port
METRICS_PORT=9090

# Enable distributed tracing
ENABLE_TRACING=false

# Jaeger endpoint (if tracing enabled)
# JAEGER_ENDPOINT=http://localhost:14268/api/traces

# =============================================================================
# Development & Testing
# =============================================================================

# Run in development mode (more verbose logging)
DEVELOPMENT=false

# Enable request/response logging
LOG_REQUESTS=false

# Mock LLM responses for testing (bypasses actual API calls)
MOCK_LLM=false

# =============================================================================
# Security Configuration
# =============================================================================

# API key for accessing the agent endpoints (optional)
# AGENT_API_KEY=your_secure_api_key

# Allowed origins for CORS (comma-separated)
# CORS_ORIGINS=http://localhost:3000,https://yourdomain.com

# Rate limiting (requests per minute per IP)
RATE_LIMIT=60

# =============================================================================
# Examples for Different Use Cases
# =============================================================================

# For development/testing:
# DEFAULT_MODEL=gpt-4o-mini
# DEFAULT_TEMPERATURE=0.0
# MAX_RETRIES=1
# DEBUG=true
# LOG_LEVEL=debug

# For production:
# DEFAULT_MODEL=gpt-4o
# DEFAULT_TEMPERATURE=0.7
# MAX_RETRIES=3
# ENABLE_METRICS=true
# LOG_LEVEL=info

# For high-volume production:
# DEFAULT_MODEL=gpt-4o-mini
# MAX_RETRIES=5
# RATE_LIMIT=300
# ENABLE_METRICS=true
# ENABLE_TRACING=true